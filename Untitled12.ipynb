{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwOwrw9R5qJhAipl1V41BP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sameepshrestha/segmentation_state_of_the_art/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq3Ck8HAB4xy"
      },
      "source": [
        "# # \n",
        "# import tensorflow as tf \n",
        "# from tensorflow.keras.layers import Input\n",
        "# inputs = Input(shape=(480,640,3))\n",
        "# model = tf.keras.applications.EfficientNetB7(include_top=False,weights=\"imagenet\",input_tensor=inputs)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPSbYPZyCIvP",
        "outputId": "683244a3-0dcb-4689-dfab-76cadcf26ac4"
      },
      "source": [
        "# model.summary()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXMEt7jkCaVW"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import cv2 as cv \n",
        "from PIL import Image, ImageDraw\n",
        "import json \n",
        "import os \n",
        "import glob \n",
        "import re \n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Reshape, Concatenate, AveragePooling2D,Input, BatchNormalization, Activation, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam \n",
        "#using tensorflow backend 16 so keras backend still 32bytes so no keras to be used\n",
        "# from keras.layers.merge import concatenate,Concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jYMHKtEL-oK"
      },
      "source": [
        "tf.keras.mixed_precision.experimental.set_policy('mixed_float16')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jl9K0TuK0kZ",
        "outputId": "3ed9cc81-da51-4e9a-f34c-e55d311c4ced"
      },
      "source": [
        "category = \"\"\"wall, floor, cabinet, bed, chair, sofa, table, door, window, bookshelf, picture, counter, blinds, desk, shelves, curtain, dresser\n",
        ", pillow, mirror, floor mat, clothes, ceiling, books, fridge, tv, paper, towel, shower curtain, box, whiteboard, person, nightstand, toilet, sink\n",
        ", lamp, bathtub, bag\"\"\"\n",
        "category = category.split(r', ')\n",
        "category_dict = {k.replace('\\n',''):category.index(k)+1 for k in category}\n",
        "print(category_dict)\n",
        "reverse_map={i:k for k,i in category_dict.items()}\n",
        "color_list=['#a0522d','#FF0000','#FFFF00','#0000F','#4B8BBE','#306998','#FFE873','#FFD43B','#646464','#5a0000','#003a27','#C0C0C0','#808080','#800000','#808000','#00FF00','#00FFFF','#008080','#000080','#FF00FF','#800080','#CD5C5C','#F08080','#FA8072','#E9967A','#FFA07A','#DC143C','#FF7F50','#FFD700','#ffffe0','#bdb76b','#228b22','#B0E0E6','#4169e1','#f0ffff','#d2691e','#BC8F8F']\n",
        "class_color_map={k:color_list[i] for i,k in enumerate(reverse_map.keys())}"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'wall': 1, 'floor': 2, 'cabinet': 3, 'bed': 4, 'chair': 5, 'sofa': 6, 'table': 7, 'door': 8, 'window': 9, 'bookshelf': 10, 'picture': 11, 'counter': 12, 'blinds': 13, 'desk': 14, 'shelves': 15, 'curtain': 16, 'dresser': 17, 'pillow': 18, 'mirror': 19, 'floor mat': 20, 'clothes': 21, 'ceiling': 22, 'books': 23, 'fridge': 24, 'tv': 25, 'paper': 26, 'towel': 27, 'shower curtain': 28, 'box': 29, 'whiteboard': 30, 'person': 31, 'nightstand': 32, 'toilet': 33, 'sink': 34, 'lamp': 35, 'bathtub': 36, 'bag': 37}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH44_RJvK9TL",
        "outputId": "9b669aef-580f-4766-9209-8df16a1eb58d"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "Image_width = 640 \n",
        "Image_height = 480 \n",
        "no_classes = len(category_dict)\n",
        "print(no_classes)\n",
        "def load_data(path, split=0.16):\n",
        "    images = sorted(glob.glob(os.path.join(path, \"image/*\")))\n",
        "    print(len(images))\n",
        "    masks = sorted(glob.glob(os.path.join(path, \"mask/*\")))\n",
        "    print(len(masks))\n",
        "    total_size = 2000\n",
        "    valid_size = int(split * total_size)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images[:2000], test_size=valid_size, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks[:2000], test_size=valid_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y)\n",
        "(train_x,train_y),(valid_x,valid_y)=load_data('/content/drive/MyDrive/depth')\n",
        "train_size=len(train_x)\n",
        "valid_size=len(valid_x)\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv.imread(path, cv.IMREAD_COLOR)\n",
        "    x = cv.resize(x,(640,480),interpolation=cv.INTER_AREA)\n",
        "    x = x/255.0\n",
        "    x=tf.cast(x,dtype=tf.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
        "    x = cv.resize(x,(640,480),interpolation=cv.INTER_NEAREST)\n",
        "    x = to_categorical(x,num_classes=no_classes+1)\n",
        "    x = x[:,:,:]\n",
        "    # x = np.expand_dims(x, axis=-1)\n",
        "    # x=np.concatenate([x,x],axis=-1)\n",
        "    x=tf.cast(x,dtype=tf.float32)\n",
        "    return x\n",
        "\n",
        "def parser(x,y):\n",
        "    def _parse(x,y):\n",
        "        x=read_image(x)\n",
        "        y=read_mask(y)\n",
        "        return x,y\n",
        "    x,y = tf.numpy_function(_parse, [x,y], [tf.float32,tf.float32])\n",
        "    x.set_shape([Image_height, Image_width, 3])\n",
        "    y.set_shape([Image_height, Image_width, no_classes+1])\n",
        "    return x,y\n",
        "    \n",
        "def tf_dataset(x, y, batch):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.map(parser)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "train_dataset=tf_dataset(train_x,train_y,batch=4)\n",
        "valid_dataset=tf_dataset(valid_x,valid_y,batch=4)\n",
        "train_dataset"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37\n",
            "8033\n",
            "7990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 480, 640, 3), (None, 480, 640, 38)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGHr-V6pFCFw",
        "outputId": "d1aaec68-58f0-438d-85db-0b63525e9c56"
      },
      "source": [
        "\n",
        "def conv_block(inputs,filters):\n",
        "  x = Conv2D(filters, (3, 3), padding=\"same\",kernel_initializer='he_normal')(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(filters, (3, 3), padding=\"same\",kernel_initializer='he_normal')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  return x \n",
        "\n",
        "def encoder(inputs):\n",
        "  num_filters = [32,64, 128,256,256]\n",
        "  x= inputs\n",
        "  for i,filters  in enumerate(num_filters):\n",
        "    x = conv_block(x,filters)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "  return x \n",
        "def decoder(x):\n",
        "  num_filters = [32,64, 128,256,256]\n",
        "  num_filters.reverse()\n",
        "  for i,filters in enumerate(num_filters):\n",
        "    print(filters)\n",
        "    x = conv_block(x,filters)\n",
        "    x = UpSampling2D((2,2))(x)\n",
        "    \n",
        "  return x\n",
        "def output_block(inputs,classes):\n",
        "    x = Conv2D(classes, (1, 1), padding=\"same\")(inputs)\n",
        "    return x\n",
        "def main_model(shape):\n",
        "  inputs = Input(shape)\n",
        "  encoded = encoder(inputs)\n",
        "  decoded = decoder(encoded)\n",
        "  output = output_block(decoded,classes=38)\n",
        "  model = Model(inputs,output)\n",
        "  return model\n",
        "\n",
        "model = main_model((480,640,3))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "256\n",
            "128\n",
            "64\n",
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5OPWbZNKuPM",
        "outputId": "dc5cae69-a28a-4527-c6ac-aab07c348f15"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, 480, 640, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_84 (Conv2D)           (None, 480, 640, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_80 (Batc (None, 480, 640, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_80 (Activation)   (None, 480, 640, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_85 (Conv2D)           (None, 480, 640, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_81 (Batc (None, 480, 640, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_81 (Activation)   (None, 480, 640, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 240, 320, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_86 (Conv2D)           (None, 240, 320, 64)      18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_82 (Batc (None, 240, 320, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_82 (Activation)   (None, 240, 320, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 240, 320, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_83 (Batc (None, 240, 320, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_83 (Activation)   (None, 240, 320, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 120, 160, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_88 (Conv2D)           (None, 120, 160, 128)     73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_84 (Batc (None, 120, 160, 128)     512       \n",
            "_________________________________________________________________\n",
            "activation_84 (Activation)   (None, 120, 160, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 120, 160, 128)     147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_85 (Batc (None, 120, 160, 128)     512       \n",
            "_________________________________________________________________\n",
            "activation_85 (Activation)   (None, 120, 160, 128)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 60, 80, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_90 (Conv2D)           (None, 60, 80, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_86 (Batc (None, 60, 80, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_86 (Activation)   (None, 60, 80, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_91 (Conv2D)           (None, 60, 80, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_87 (Batc (None, 60, 80, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_87 (Activation)   (None, 60, 80, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_92 (Conv2D)           (None, 30, 40, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 30, 40, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_88 (Activation)   (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 30, 40, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 30, 40, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_89 (Activation)   (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 15, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 15, 20, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 15, 20, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_90 (Activation)   (None, 15, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 15, 20, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 15, 20, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_91 (Activation)   (None, 15, 20, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_20 (UpSampling (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 30, 40, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 30, 40, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_92 (Activation)   (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 30, 40, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 30, 40, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation_93 (Activation)   (None, 30, 40, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_21 (UpSampling (None, 60, 80, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_98 (Conv2D)           (None, 60, 80, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 60, 80, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_94 (Activation)   (None, 60, 80, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_99 (Conv2D)           (None, 60, 80, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 60, 80, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_95 (Activation)   (None, 60, 80, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_22 (UpSampling (None, 120, 160, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_100 (Conv2D)          (None, 120, 160, 64)      73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_96 (Batc (None, 120, 160, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_96 (Activation)   (None, 120, 160, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_101 (Conv2D)          (None, 120, 160, 64)      36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 120, 160, 64)      256       \n",
            "_________________________________________________________________\n",
            "activation_97 (Activation)   (None, 120, 160, 64)      0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_23 (UpSampling (None, 240, 320, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_102 (Conv2D)          (None, 240, 320, 32)      18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 240, 320, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_98 (Activation)   (None, 240, 320, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_103 (Conv2D)          (None, 240, 320, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_99 (Batc (None, 240, 320, 32)      128       \n",
            "_________________________________________________________________\n",
            "activation_99 (Activation)   (None, 240, 320, 32)      0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_24 (UpSampling (None, 480, 640, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_104 (Conv2D)          (None, 480, 640, 38)      1254      \n",
            "=================================================================\n",
            "Total params: 5,306,822\n",
            "Trainable params: 5,300,934\n",
            "Non-trainable params: 5,888\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzHtdx6wNevn"
      },
      "source": [
        "#loss function \n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import CSVLogger , TensorBoard, LearningRateScheduler\n",
        "filepath= '/content/drive/MyDrive/JUNE11training/checkpoint/'\n",
        "filepath2 = '/content/drive/MyDrive/JUNE11training/tensorboard'\n",
        "# callbacks = [\n",
        "#     EarlyStopping(monitor='val_custom_metrics', patience=5),\n",
        "#     ModelCheckpoint(filepath, monitor='val_custom_metrics',verbose=1,mode='max',save_best_only=True),    \n",
        "#     TensorBoard(filepath2,histogram_freq=1),\n",
        "#     # LearningRateScheduler(lrfn, verbose = True)\n",
        "    \n",
        "# ]\n",
        "def binary_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_true,y_pred):\n",
        "    # y_pred = tf.math.exp(y_pred)\n",
        "    # sum = tf.math.reduce_sum(y_pred, axis = [-1],keepdims=True)\n",
        "    # y_pred = tf.divide(y_pred,sum)\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    y_true, y_pred, axis=-1, name=None)\n",
        "    return loss\n",
        "\n",
        "def custom_metrics(y_true,y_pred,smooth=1):\n",
        "    y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "    # y_pred = tf.expand_dims(y_pred,axis=-1)\n",
        "    print(y_pred)\n",
        "    y_pred = tf.one_hot(y_pred,38)\n",
        "    y_true = y_true[:,:,:,1:38]\n",
        "    y_pred= y_pred[:,:,:,1:38]\n",
        "    intersection  = tf.math.reduce_sum((y_true*y_pred),axis = [1,2,3])\n",
        "    union = tf.math.reduce_sum(y_true,axis=[1,2,3])+tf.math.reduce_sum(y_pred,axis = [1,2,3]) - intersection \n",
        "    return tf.math.reduce_mean((intersection+smooth)/(union+smooth),axis=0)\n",
        "smooth = 1\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "# model=tf.keras.models.load_model('/content/drive/MyDrive/JUNE11training/checkpoint/',custom_objects={\n",
        "#         'cross_entropy_loss': cross_entropy_loss ,'dice_coef': dice_coef,'custom_metrics':custom_metrics})\n",
        "\n",
        "model.compile(loss = cross_entropy_loss,optimizer=Adam(learning_rate =.001),metrics=[custom_metrics])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOu4GKVVOpis",
        "outputId": "35d06681-1622-459c-c077-f40651835bb7"
      },
      "source": [
        "t_steps=train_size//4\n",
        "v_steps=valid_size//4\n",
        "model.fit(train_dataset, \n",
        "                        epochs = 12,\n",
        "                        steps_per_epoch = t_steps,\n",
        "                        validation_data = valid_dataset,\n",
        "                        validation_steps = v_steps,\n",
        "                        # callbacks=callbacks\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "Tensor(\"ArgMax:0\", shape=(None, 480, 640), dtype=int64)\n",
            "Tensor(\"ArgMax:0\", shape=(None, 480, 640), dtype=int64)\n",
            "420/420 [==============================] - ETA: 0s - loss: 2.2688 - custom_metrics: 0.1433Tensor(\"ArgMax:0\", shape=(None, 480, 640), dtype=int64)\n",
            "420/420 [==============================] - 955s 2s/step - loss: 2.2688 - custom_metrics: 0.1433 - val_loss: 1.9889 - val_custom_metrics: 0.1434\n",
            "Epoch 2/12\n",
            "420/420 [==============================] - 183s 436ms/step - loss: 1.9478 - custom_metrics: 0.1595 - val_loss: 2.4177 - val_custom_metrics: 0.0923\n",
            "Epoch 3/12\n",
            "420/420 [==============================] - 202s 481ms/step - loss: 1.9009 - custom_metrics: 0.1783 - val_loss: 2.0099 - val_custom_metrics: 0.1108\n",
            "Epoch 4/12\n",
            "420/420 [==============================] - 183s 437ms/step - loss: 1.8700 - custom_metrics: 0.1849 - val_loss: 1.9232 - val_custom_metrics: 0.1326\n",
            "Epoch 5/12\n",
            "420/420 [==============================] - 182s 434ms/step - loss: 1.8405 - custom_metrics: 0.1941 - val_loss: 1.8061 - val_custom_metrics: 0.2199\n",
            "Epoch 6/12\n",
            "420/420 [==============================] - ETA: 0s - loss: 1.8120 - custom_metrics: 0.2020"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8GwH6gcPnRZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}